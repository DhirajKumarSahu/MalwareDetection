{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suburban-control",
   "metadata": {},
   "source": [
    "### For this assignment, because of resourse constraints I have used Continous Learning techinque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Process# this is used for multithreading\n",
    "import multiprocessing\n",
    "import codecs# this is used for file operations \n",
    "import random as r\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promising-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import array\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import hstack\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dried-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=\"0,1,2,3,4,5,6,7,8,9,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hairy-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_feature_names=[]\n",
    "for i in f.split(\",\"):\n",
    "    for j in f.split(\",\"):\n",
    "        bigram_feature_names.append(i+\" \"+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abroad-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Count Vectorizer for chunk 0\n",
      "Started ASM Byte features for chunk 0\n",
      "[19:32:23] WARNING: D:\\bld\\xgboost-split_1631904903843\\work\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Started Count Vectorizer for chunk 1\n",
      "Started ASM Byte features for chunk 1\n",
      "[21:08:44] WARNING: D:\\bld\\xgboost-split_1631904903843\\work\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Started Count Vectorizer for chunk 2\n",
      "Started ASM Byte features for chunk 2\n",
      "[22:44:21] WARNING: D:\\bld\\xgboost-split_1631904903843\\work\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Started Count Vectorizer for chunk 3\n",
      "Started ASM Byte features for chunk 3\n",
      "[23:44:24] WARNING: D:\\bld\\xgboost-split_1631904903843\\work\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xcv=[]\n",
    "ycv=[]\n",
    "xtest=[]\n",
    "ytest=[]\n",
    "cs=3000 # Chunk Size\n",
    "for j,chunk in enumerate(pd.read_csv(\"malware_data.csv\",chunksize=cs)):\n",
    "    \n",
    "    if j==0:\n",
    "        start=j\n",
    "        end=j+cs\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(2,2),vocabulary=bigram_feature_names)  # bigram features for given chunk\n",
    "    files = os.listdir('F:\\\\byteFiles')\n",
    "    \n",
    "    k=0\n",
    "    print(\"Started Count Vectorizer for chunk\",j)\n",
    "    for file in files[start:end]:\n",
    "        if(file.endswith(\"txt\")):\n",
    "            with open('F:byteFiles/'+file,\"r\") as byte_file:\n",
    "                content=[byte_file.read()]\n",
    "                if k==0:\n",
    "                    bi_grams=vectorizer.transform(content)\n",
    "                    k=1\n",
    "                else:\n",
    "                    bi_grams = sp.vstack((bi_grams,vectorizer.transform(content)))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    asmfiless=os.listdir(\"F:\\\\asmfiles\") # bytefeatures for the given chunk\n",
    "    asmfiles=asmfiless[start:end]\n",
    "    \n",
    "    asm_matrix=np.zeros((len(asmfiles),800))\n",
    "    print(\"Started ASM Byte features for chunk\",j)\n",
    "    for i, asmfile in enumerate(asmfiles):\n",
    "        filename=asmfile.split('.')[0]\n",
    "        #files.append(filename)\n",
    "        file=open(\"F:asmFiles/\"+asmfile,'rb')\n",
    "        #print(file.read())\n",
    "        arr=array.array('B')\n",
    "        arr.frombytes(file.read())\n",
    "        asm_matrix[i,:]=arr[:800]\n",
    "       \n",
    "        \n",
    "    result_y = chunk['class']\n",
    "    result_x = chunk.drop(['ID','rtn','.BSS:','.CODE','class'], axis=1)\n",
    "  \n",
    "    \n",
    "    X_train = hstack((result_x,asm_matrix,bi_grams)).tocsr()\n",
    "    X_train, X_test_merge, y_train, y_test_merge = train_test_split(X_train,result_y,stratify=result_y,test_size=0.20)\n",
    "    X_train_merge, X_cv_merge, y_train_merge, y_cv_merge = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)\n",
    "    xcv.append(X_cv_merge)\n",
    "    ycv.append(y_cv_merge)\n",
    "    xtest.append(X_test_merge)\n",
    "    ytest.append(y_test_merge)\n",
    "    \n",
    "    x_cfl=XGBClassifier(n_estimators=1000,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "    \n",
    "    if j==0:\n",
    "        x_cf0=XGBClassifier(n_estimators=1000,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "        model_0=x_cf0.fit(X_train_merge,y_train_merge,verbose=True)\n",
    "        model_0.save_model('model_0.model')\n",
    "        \n",
    "    elif j==1:\n",
    "        x_cf1=XGBClassifier(n_estimators=1000,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "        model_1=x_cf1.fit(X_train_merge,y_train_merge,verbose=True,xgb_model='model_0.model')\n",
    "        model_1.save_model('model_1.model')\n",
    "    \n",
    "    elif j==2:\n",
    "        x_cf2=XGBClassifier(n_estimators=1000,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "        model_2=x_cf2.fit(X_train_merge,y_train_merge,verbose=True,xgb_model='model_1.model')\n",
    "        model_2.save_model('model_2.model')\n",
    "        \n",
    "    elif j==3: \n",
    "        x_cf3=XGBClassifier(n_estimators=1000,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "        model_3=x_cf3.fit(X_train_merge,y_train_merge,verbose=True,xgb_model='model_2.model')\n",
    "        model_3.save_model('model_3.model')\n",
    "        \n",
    "    start=end\n",
    "    end=end+3000\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "encouraging-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "cross_validate_x=vstack(xcv)\n",
    "cross_validate_y=pd.concat([ycv[0],ycv[1],ycv[2],ycv[3]])\n",
    "\n",
    "test_x=vstack(xtest)\n",
    "test_y=pd.concat([ytest[0],ytest[1],ytest[2],ytest[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "african-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation log loss is: 0.016105834740058167\n"
     ]
    }
   ],
   "source": [
    "predict_y=model_3.predict_proba(cross_validate_x)\n",
    "print(\"The cross validation log loss is:\",log_loss(cross_validate_y, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "imperial-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test log loss is: 0.007485997739975661\n"
     ]
    }
   ],
   "source": [
    "predict_y=model_3.predict_proba(test_x)\n",
    "print(\"The test log loss is:\",log_loss(test_y, predict_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
